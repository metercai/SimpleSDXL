{
  "3": {
    "inputs": {
      "seed": 976512205133628,
      "steps": 28,
      "cfg": 1,
      "sampler_name": "euler",
      "scheduler": "simple",
      "denoise": 1,
      "model": [
        "101",
        0
      ],
      "positive": [
        "47",
        0
      ],
      "negative": [
        "47",
        1
      ],
      "latent_image": [
        "87",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "KSampler"
    }
  },
  "6": {
    "inputs": {
      "text": "professional raw photo , realistic, analog photo, photograph of a 18-year-old young man, portraitsolo, looking at viewer, black hair,  upper body,  male focus, black eyes,",
      "clip": [
        "113",
        0
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "prompt"
    }
  },
  "7": {
    "inputs": {
      "text": "Noise, spots, cracks",
      "clip": [
        "113",
        0
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "8": {
    "inputs": {
      "samples": [
        "3",
        0
      ],
      "vae": [
        "27",
        0
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE Decode"
    }
  },
  "11": {
    "inputs": {
      "guidance": 3.5,
      "conditioning": [
        "6",
        0
      ]
    },
    "class_type": "FluxGuidance",
    "_meta": {
      "title": "FluxGuidance"
    }
  },
  "27": {
    "inputs": {
      "vae_name": "ae.safetensors"
    },
    "class_type": "VAELoader",
    "_meta": {
      "title": "Load VAE"
    }
  },
  "30": {
    "inputs": {
      "image": "åŽŸå§‹ç…§ç‰‡.png",
      "upload": "image"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "i2i_ip_image1"
    }
  },
  "47": {
    "inputs": {
      "strength": 0.7000000000000001,
      "start_percent": 0,
      "end_percent": 1,
      "positive": [
        "11",
        0
      ],
      "negative": [
        "7",
        0
      ],
      "control_net": [
        "48",
        0
      ],
      "image": [
        "179",
        0
      ],
      "vae": [
        "27",
        0
      ]
    },
    "class_type": "ControlNetApplyAdvanced",
    "_meta": {
      "title": "Apply ControlNet"
    }
  },
  "48": {
    "inputs": {
      "control_net_name": "flux.1-dev_controlnet_upscaler.safetensors"
    },
    "class_type": "ControlNetLoader",
    "_meta": {
      "title": "Load ControlNet Model"
    }
  },
  "87": {
    "inputs": {
      "pixels": [
        "153",
        0
      ],
      "vae": [
        "27",
        0
      ]
    },
    "class_type": "VAEEncode",
    "_meta": {
      "title": "VAE Encode"
    }
  },
  "101": {
    "inputs": {
      "unet_name": "flux-hyp8-Q5_K_M.gguf"
    },
    "class_type": "UnetLoaderGGUF",
    "_meta": {
      "title": "Unet Loader (GGUF)"
    }
  },
  "113": {
    "inputs": {
      "clip_name1": "clip_l.safetensors",
      "clip_name2": "t5xxl_fp16.safetensors",
      "type": "flux"
    },
    "class_type": "DualCLIPLoader",
    "_meta": {
      "title": "DualCLIPLoader"
    }
  },
  "123": {
    "inputs": {
      "width": 1024,
      "height": 1024,
      "interpolation": "lanczos",
      "method": "keep proportion",
      "condition": "always",
      "multiple_of": 8,
      "image": [
        "180",
        0
      ]
    },
    "class_type": "ImageResize+",
    "_meta": {
      "title": "ðŸ”§ Image Resize"
    }
  },
  "138": {
    "inputs": {
      "images": [
        "168",
        0
      ]
    },
    "class_type": "SaveImageWebsocket",
    "_meta": {
      "title": "SaveImageWebsocket"
    }
  },
  "153": {
    "inputs": {
      "blur_radius": 2,
      "sigma": 2,
      "image": [
        "123",
        0
      ]
    },
    "class_type": "ImageBlur",
    "_meta": {
      "title": "Image Blur"
    }
  },
  "166": {
    "inputs": {
      "seed": 444275945157085,
      "steps": 8,
      "cfg": 0.8,
      "sampler_name": "euler_ancestral_cfg_pp",
      "scheduler": "sgm_uniform",
      "denoise": 0.45,
      "model": [
        "174",
        0
      ],
      "positive": [
        "169",
        0
      ],
      "negative": [
        "169",
        1
      ],
      "latent_image": [
        "171",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "KSampler"
    }
  },
  "167": {
    "inputs": {
      "text": "professional raw photo , realistic, analog photo, A 18-year-old Chinese  young man",
      "clip": [
        "173",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "prompt"
    }
  },
  "168": {
    "inputs": {
      "samples": [
        "166",
        0
      ],
      "vae": [
        "173",
        2
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE Decode"
    }
  },
  "169": {
    "inputs": {
      "strength": 0.8,
      "start_percent": 0,
      "end_percent": 1,
      "positive": [
        "167",
        0
      ],
      "negative": [
        "177",
        0
      ],
      "control_net": [
        "176",
        0
      ],
      "image": [
        "179",
        0
      ],
      "vae": [
        "173",
        2
      ]
    },
    "class_type": "ControlNetApplyAdvanced",
    "_meta": {
      "title": "Apply ControlNet"
    }
  },
  "170": {
    "inputs": {
      "control_net_name": "xinsir_cn-union-sdxl-1.0_promax.safetensors"
    },
    "class_type": "ControlNetLoader",
    "_meta": {
      "title": "Load ControlNet Model"
    }
  },
  "171": {
    "inputs": {
      "pixels": [
        "182",
        0
      ],
      "vae": [
        "173",
        2
      ]
    },
    "class_type": "VAEEncode",
    "_meta": {
      "title": "VAE Encode"
    }
  },
  "172": {
    "inputs": {
      "model_name": "4xNomos8kSCHAT-L.pth"
    },
    "class_type": "UpscaleModelLoader",
    "_meta": {
      "title": "Load Upscale Model"
    }
  },
  "173": {
    "inputs": {
      "ckpt_name": "LEOSAM_HelloWorldXL_70.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "base_model"
    }
  },
  "174": {
    "inputs": {
      "lora_name": "Hyper-SDXL-8steps-lora.safetensors",
      "strength_model": 1,
      "model": [
        "173",
        0
      ]
    },
    "class_type": "LoraLoaderModelOnly",
    "_meta": {
      "title": "LoraLoaderModelOnly"
    }
  },
  "176": {
    "inputs": {
      "type": "auto",
      "control_net": [
        "170",
        0
      ]
    },
    "class_type": "SetUnionControlNetType",
    "_meta": {
      "title": "SetUnionControlNetType"
    }
  },
  "177": {
    "inputs": {
      "text": "Noise, spots, cracks",
      "clip": [
        "173",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "negative_prompt"
    }
  },
  "179": {
    "inputs": {
      "width": 512,
      "height": 512,
      "interpolation": "lanczos",
      "method": "keep proportion",
      "condition": "always",
      "multiple_of": 8,
      "image": [
        "30",
        0
      ]
    },
    "class_type": "ImageResize+",
    "_meta": {
      "title": "ðŸ”§ Image Resize"
    }
  },
  "180": {
    "inputs": {
      "upscale_model": [
        "172",
        0
      ],
      "image": [
        "179",
        0
      ]
    },
    "class_type": "ImageUpscaleWithModel",
    "_meta": {
      "title": "Upscale Image (using Model)"
    }
  },
  "181": {
    "inputs": {
      "width": [
        "191",
        0
      ],
      "height": [
        "191",
        0
      ],
      "interpolation": "lanczos",
      "method": "keep proportion",
      "condition": "always",
      "multiple_of": 8,
      "image": [
        "190",
        0
      ]
    },
    "class_type": "ImageResize+",
    "_meta": {
      "title": "ðŸ”§ Image Resize"
    }
  },
  "182": {
    "inputs": {
      "blur_radius": 4,
      "sigma": 2,
      "image": [
        "181",
        0
      ]
    },
    "class_type": "ImageBlur",
    "_meta": {
      "title": "Image Blur"
    }
  },
  "189": {
    "inputs": {
      "model_name": "4xNomos8kSCHAT-L.pth"
    },
    "class_type": "UpscaleModelLoader",
    "_meta": {
      "title": "Load Upscale Model"
    }
  },
  "190": {
    "inputs": {
      "upscale_model": [
        "189",
        0
      ],
      "image": [
        "8",
        0
      ]
    },
    "class_type": "ImageUpscaleWithModel",
    "_meta": {
      "title": "Upscale Image (using Model)"
    }
  },
  "191": {
    "inputs": {
      "value": 1536
    },
    "class_type": "easy int",
    "_meta": {
      "title": "aspect_ratios_width"
    }
  }
}